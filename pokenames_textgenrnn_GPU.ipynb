{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pokenames textgenrnn  GPU",
      "provenance": [],
      "collapsed_sections": [
        "IokF0YBfzA6K"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbadams/pokegen/blob/ipython/pokenames_textgenrnn_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Textgen-rnn Pokemon\n",
        "\n",
        "by Trevor Adams\n",
        "\n",
        "based on [textgen-rnn](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK#scrollTo=aeXshJM-Cuaf) by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "For more about textgenrnn, you can visit [this GitHub repository](https://github.com/minimaxir/textgenrnn).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qyjWCs8kbvn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IokF0YBfzA6K"
      },
      "source": [
        "### Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "outputId": "e3f63eae-9b74-4420-88b2-93f228e03db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install -q textgenrnn\n",
        "from google.colab import files\n",
        "from textgenrnn import textgenrnn\n",
        "from datetime import datetime\n",
        "import os\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import random\n",
        "import pprint\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "SAMPLE_DATA_FILE_NAME = \"poke_data.txt\"\n",
        "\n",
        "# link drive so we can pull base data automatically/save results\n",
        "# can also upload manually before run\n",
        "drive.mount('/content/drive')\n",
        "# IMPORTANT! CLICK ON THE KEY ITSELF WHEN COPYING, DO NOT CLICK COPY BUTTON\n",
        "# BUTTON DOES NOT WORK !!?!\n",
        "\n",
        "# Set up library params\n",
        "\n",
        "model_cfg = {\n",
        "    'word_level': False,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
        "    'rnn_size': 256,   # number of LSTM cells of each layer (128/256 recommended)\n",
        "    'rnn_layers': 2,   # number of LSTM layers (>=2 recommended)\n",
        "    'rnn_bidirectional': True,   # consider text both forwards and backward, can give a training boost\n",
        "    'max_length': 40,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
        "    'max_words': 10000   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
        "}\n",
        " # @markdown set higher to train the model for longer between sample generations\n",
        "num_epochs = 1 #@param {type: \"number\"}\n",
        " # @markdown set higher to execute more train/generate cycles\n",
        "# todo call them runs\n",
        "total_rounds = 10 #@param {type: \"number\"}\n",
        " # @markdown proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
        "train_size = 0.9  #@param {type: \"number\"}\n",
        " # @markdown ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
        "dropout = 0.1 #@param {type: \"number\"}\n",
        "gen_epochs = 99999 # library generates sample text from model after given number of epochs\n",
        "train_cfg = {\n",
        "    'line_delimited': True,   # set to True if each text has its own line in the source file\n",
        "    'num_epochs': num_epochs,  \n",
        "    'gen_epochs': gen_epochs,  \n",
        "    'train_size': train_size,  \n",
        "    'dropout': dropout,  \n",
        "    'validation': True,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
        "    'is_csv': False,   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
        "    'new_model':False\n",
        "}\n",
        "#@markdown change to set file name of resulting trained models/texts\n",
        "model_name = 'tpoke4b'   # @param {type: \"string\"}\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/My Drive/\"\n",
        "cur_round = 0\n",
        "\n",
        "# prepare sample text\n",
        "file_name = SAMPLE_DATA_FILE_NAME\n",
        "if not os.path.exists(file_name):\n",
        "  uploaded = files.upload()\n",
        "if not os.path.exists(file_name):\n",
        "  raise FileNotFoundError(\"Need a source file for data!!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAMeBsvfMQV"
      },
      "source": [
        "### Uh, everything else"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPVARZHNfPgn",
        "cellView": "both",
        "outputId": "0c124fba-a69d-44a8-faab-2666e1235bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "checkpoint_file_templates = ['{}_weights.hdf5', '{}_vocab.json','{}_config.json']\n",
        "\n",
        "def all_to_gdrive(fnames, overwrite=False):\n",
        "  for fname in fnames:\n",
        "    if  overwrite or not os.path.isfile(os.path.join(DRIVE_ROOT, fname)):\n",
        "      print(\"copied {}\".format(fname))\n",
        "      shutil.copyfile(fname, DRIVE_ROOT + fname)\n",
        "    else:\n",
        "      print(\"skipped {}\".format(fname))\n",
        "\n",
        "def save_checkpoint_to_drive(model_name, add_labeled_copy=True):\n",
        "  fnames = []\n",
        "  copy_names = []\n",
        "  for t in checkpoint_file_templates:\n",
        "    fname = t.format(model_name)\n",
        "    fnames.append(fname)\n",
        "    if add_labeled_copy:\n",
        "      rounds = cur_round * num_epochs\n",
        "      copyname = t.format(\"{}_{}\".format(model_name, rounds))\n",
        "      shutil.copyfile(fname, copyname)\n",
        "      fnames.append(copyname)\n",
        "      copy_names.append(copyname)\n",
        "  all_to_gdrive(fnames, True)\n",
        "  for copyname in copy_names:\n",
        "    os.remove(copyname)\n",
        "\n",
        "logs = []\n",
        "def log(msg):\n",
        "  logs.append(msg)\n",
        "  print(msg)\n",
        "\n",
        "import json\n",
        "# this temperature schedule cycles between 1 very unexpected token, 1 unexpected token, 2 expected tokens, repeat.\n",
        "# changing the temperature schedule can result in wildly different output!\n",
        "# temperature = [1.0, 0.5, 0.2, 0.2]  \n",
        "\n",
        "# if train_cfg['line_delimited']:\n",
        "#   n = 1000\n",
        "#   max_gen_length = 60 if model_cfg['word_level'] else 300\n",
        "# else:\n",
        "#   n = 1\n",
        "#   max_gen_length = 2000 if model_cfg['word_level'] else 1000\n",
        "# timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "def make_samples(temp_pattern=[0.5], length=400, sample_num=500, \n",
        "                prefix=\"<|startoftext|>\"): # if you want each generated text to start with a given seed text):\n",
        "  samples = \"\"\n",
        "  for n in range(sample_num):\n",
        "    samples = samples + textgen.generate(temperature=temp_pattern,\n",
        "                          prefix=prefix,\n",
        "                          n=sample_num,\n",
        "                          return_as_list=True,\n",
        "                          max_gen_length=length)[0]\n",
        "  return samples\n",
        "\n",
        "\n",
        "def write_sample_header(header_file_name, temperature, prefix):\n",
        "      # add param header\n",
        "      gen_header = {\"model_name\":model_name,\n",
        "                      \"temperature\": str(temperature),\n",
        "                      \"prefix\": prefix,\n",
        "                      \"model_cfg\":model_cfg, \n",
        "                      \"train_cfg\":train_cfg}\n",
        "      with open(header_file_name, \"w\") as gen_header_file:\n",
        "        gen_header_file.write(json.dumps(gen_header, indent=4))\n",
        "        print(\"wrote gen header to \" + header_file_name)\n",
        "\n",
        "def permute_samples(temps=[[0.2],[0.5], [0.75], [1.0], [1.0, 0.5, 0.2, 0.2]], \n",
        "                 prefix=\"<|startoftext|>\",\n",
        "                 length=1000, sample_num=1, header=True, download=True):\n",
        "  for temperature in temps:\n",
        "      print(\"Sampling temp pattern {}\".format(temperature))\n",
        "      starttime = time.time()\n",
        "      file_prefix = \"{}_{}_{}_{}\".format(\n",
        "          model_name, cur_round * num_epochs, int(starttime), \n",
        "          \"temp\"+ \",\".join(map(lambda t: str(t).lstrip(\"0.\"), temperature)))\n",
        "      samples_file_name = \"{}.txt\".format(file_prefix)\n",
        "      header_file_name = \"{}.json\".format(file_prefix)\n",
        "\n",
        "      samples = make_samples(\n",
        "          temperature, \n",
        "          prefix=prefix, \n",
        "          length=length, \n",
        "          sample_num=sample_num)\n",
        "      sample_lines = samples.split(\"\\n\")\n",
        "\n",
        "      with open(samples_file_name, \"w\") as samples_file:\n",
        "        samples_file.write(samples)\n",
        "        print(\"wrote {} samples to {}\".format(str(len(sample_lines)), samples_file_name))\n",
        "      if header:\n",
        "        write_sample_header(header_file_name, temperature, prefix)\n",
        "        all_to_gdrive([header_file_name, samples_file_name])\n",
        "      else:\n",
        "        all_to_gdrive([samples_file_name])\n",
        "      print(\"took {}s\".format(time.time() - starttime))\n",
        "\n",
        "def have_model(model_name=model_name):\n",
        "  for t in checkpoint_file_templates:\n",
        "    if not os.path.exists(t.format(model_name)):\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling temp pattern [0.5, 0.7]\n",
            "wrote 1 samples to tpoke4b_1_1601341530_temp5,7.txt\n",
            "wrote gen header to tpoke4b_1_1601341530_temp5,7.json\n",
            "copied tpoke4b_1_1601341530_temp5,7.json\n",
            "copied tpoke4b_1_1601341530_temp5,7.txt\n",
            "took 3.1607608795166016s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qda12ykkBjZ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpeBo5eQVa0p"
      },
      "source": [
        "textgen = textgenrnn(name=model_name)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBVgeB2ay8lR",
        "outputId": "0055d24e-120f-45b8-b278-66484ab6e77f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "# start the actual training.  Keras's CuDNN layers make training on GPU fast.\n",
        "# Ideally, you want a training loss less than `1.0` in order \n",
        "# for the model to create sensible text consistently.\n",
        "\n",
        "\n",
        "while cur_round < total_rounds:\n",
        "  cur_round = cur_round + 1\n",
        "  print(\"Starting round {} of {}.\".format(cur_round, total_rounds))\n",
        "  train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "  should_start_fresh = train_cfg['new_model'] or not have_model()\n",
        "  if should_start_fresh:\n",
        "    print(\"Initializing new model {}.\".format(model_name))\n",
        "  train_function(\n",
        "      file_path=file_name,\n",
        "      new_model=should_start_fresh,\n",
        "      num_epochs=train_cfg['num_epochs'],\n",
        "      gen_epochs=train_cfg['gen_epochs'],\n",
        "      batch_size=2048,\n",
        "      train_size=train_cfg['train_size'],\n",
        "      dropout=train_cfg['dropout'],\n",
        "      validation=train_cfg['validation'],\n",
        "      is_csv=train_cfg['is_csv'],\n",
        "      rnn_layers=model_cfg['rnn_layers'],\n",
        "      rnn_size=model_cfg['rnn_size'],\n",
        "      rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "      max_length=model_cfg['max_length'],\n",
        "      dim_embeddings=100,\n",
        "      word_level=model_cfg['word_level'])\n",
        "\n",
        "  save_checkpoint_to_drive(model_name)\n",
        "  permute_samples(length=3000)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting round 1 of 10.\n",
            "Initializing new model tpoke4b.\n",
            "1,785 texts collected.\n",
            "Training new model w/ 2-layer, 256-cell Bidirectional LSTMs\n",
            "Training on 475,692 character sequences.\n",
            "Epoch 1/1\n",
            " 12/232 [>.............................] - ETA: 2:52 - loss: 6.0648"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-90bef5269dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mdim_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       word_level=model_cfg['word_level'])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0msave_checkpoint_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mtrain_from_file\u001b[0;34m(self, file_path, header, delim, new_model, context, is_csv, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             self.train_new_model(\n\u001b[0;32m--> 322\u001b[0;31m                 texts, context_labels=context_labels, **kwargs)\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mtrain_new_model\u001b[0;34m(self, texts, context_labels, num_epochs, gen_epochs, batch_size, dropout, validation, save_epochs, multi_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                             \u001b[0msave_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mmulti_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"textgenrnn_weights_saved.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mtrain_on_texts\u001b[0;34m(self, texts, context_labels, batch_size, num_epochs, verbose, new_model, gen_epochs, train_size, max_gen_length, validation, dropout, via_new_model, save_epochs, multi_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m                               \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                               )\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "# Generate \n",
        "You can download a large amount of generated text from your model with the cell below! Rerun the cell as many times as you want for even more text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "outputId": "d6cb9d46-29fc-45ec-9b17-fb519f67d12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "permute_samples([0.5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling temp pattern 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7d5e14c5fbdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpermute_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-3d11bd865e13>\u001b[0m in \u001b[0;36mpermute_samples\u001b[0;34m(temps, prefix, length, sample_num, header, download)\u001b[0m\n\u001b[1;32m     74\u001b[0m       file_prefix = \"{}_{}_{}_{}\".format(\n\u001b[1;32m     75\u001b[0m           \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_round\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m           \"temp\"+ \",\".join(temperature))\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0msamples_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mheader_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Download Models\n",
        "\n",
        "You can download the weights and configuration files in the cell below, allowing you recreate the model on your own computer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "outputId": "584a5e02-cea8-4203-a06f-dccf7e3cc04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "files.download('{}_weights.hdf5'.format(model_name))\n",
        "files.download('{}_vocab.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bce115fb-182e-46c0-8dce-4df25a51b354\", \"pokenames_layers2-256_25_train05_weights.hdf5\", 9695640)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_12b33568-a97c-47e6-95b5-a254c9fe8b26\", \"pokenames_layers2-256_25_train05_vocab.json\", 777)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7b330aa7-c375-4e53-95d0-92ab60edccd3\", \"pokenames_layers2-256_25_train05_config.json\", 244)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "To recreate the model on your own computer, after installing textgenrnn and TensorFlow, you can create a Python script with:\n",
        "\n",
        "```\n",
        "from textgenrnn import textgenrnn\n",
        "textgen = textgenrnn(weights_path='colaboratory_weights.hdf5',\n",
        "                       vocab_path='colaboratory_vocab.json',\n",
        "                       config_path='colaboratory_config.json')\n",
        "                       \n",
        "textgen.generate_samples(max_gen_length=1000)\n",
        "textgen.generate_to_file('textgenrnn_texts.txt', max_gen_length=1000)\n",
        "```\n",
        "\n",
        "Have fun with your new model! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Zjtsb_Dgj-"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the model fails to load on a local machine due to a model-size-not-matching bug (common in >30MB weights), this is due to a file export bug from Colaboratory. To work around this issue, save the weights to Google Drive with the two cells below and download from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-IzscxUHmAB"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR4_XJpfKAIn",
        "outputId": "a9b2914a-ba83-4042-e86d-5c6943058897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "uploaded = drive.CreateFile({'title': '{}_weights.hdf5'.format(model_name)})\n",
        "uploaded.SetContentFile('{}_weights.hdf5'.format(model_name))\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 10Wiez4To7JJJaXnMkfJJsQ72FmH23qvV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}